You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Shakespeare dataset processing:
  Full text length: 1,003,854 characters
  Chunk size: 256, Overlap: 255
  Total chunks created: 1003600
  Training chunks: 903240
  Validation chunks: 100360
Final dataset sizes:
  Training examples: 2352
  Validation examples: 262
--- Example 0 ---
Input IDs  : [1589, 263, 270, 3389, 1717, 14892, 272, 373, 263, 286, 579, 29892, 13, 28181, 29892, 411, 1432, 18778, 29892, 304, 260, 15563, 1623, 13, 797, 517, 278, 18409, 12580, 1379, 310, 278, 6483, 29889, 13, 13, 3927, 29963, 6670, 29901, 13, 27796, 29892, 2041, 29892, 13916, 29936, 525, 28898, 6579, 2222, 304, 5566, 8342, 29889, 13, 13, 15715, 1254, 4214, 29903, 29901, 13, 29949, 6668, 1486, 6123, 29991, 27788, 519, 5408, 29991, 13, 29902, 27953, 267, 29891, 278, 8866, 1319, 29915, 303, 931, 304, 14904, 13, 7058, 3926, 281, 10301, 287, 5046, 23870, 1106, 29915, 29881, 2501, 29889, 13, 27796, 29892, 3275, 592, 304, 278, 2908, 29936, 11460, 1075, 590, 2343, 29889, 13, 15597, 17819, 472, 592, 393, 21734, 4091, 367, 7123, 29889, 13, 13, 29954, 3927, 29965, 4741, 1254, 1001, 29901, 13, 27796, 29892, 22618, 29892, 508, 303, 12595, 439, 1296, 29892, 322, 1735, 13874, 12384, 29892, 13, 29924, 332, 672, 13874, 16172, 297, 278, 7256, 310, 263, 1734, 29892, 13, 2855, 769, 3380, 1449, 29892, 29871, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]

 Decoded in :  ke a drunken sailor on a mast,
Ready, with every nod, to tumble down
Into the fatal bowels of the deep.

LOVEL:
Come, come, dispatch; 'tis bootless to exclaim.

HASTINGS:
O bloody Richard! miserable England!
I prophesy the fearful'st time to thee
That ever wretched age hath look'd upon.
Come, lead me to the block; bear him my head.
They smile at me that shortly shall be dead.

GLOUCESTER:
Come, cousin, canst thou quake, and change thy colour,
Murder thy breath in the middle of a word,
And then begin again, </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>

 Decoded lbl: ke a drunken sailor on a mast,
Ready, with every nod, to tumble down
Into the fatal bowels of the deep.

LOVEL:
Come, come, dispatch; 'tis bootless to exclaim.

HASTINGS:
O bloody Richard! miserable England!
I prophesy the fearful'st time to thee
That ever wretched age hath look'd upon.
Come, lead me to the block; bear him my head.
They smile at me that shortly shall be dead.

GLOUCESTER:
Come, cousin, canst thou quake, and change thy colour,
Murder thy breath in the middle of a word,
And then begin again, 

--- Example 1 ---
Input IDs  : [29871, 299, 6411, 1516, 7436, 304, 29892, 366, 29892, 3447, 23222, 306, 2360, 13, 29928, 15274, 596, 6721, 29901, 2125, 596, 7348, 310, 1906, 13, 7058, 1900, 508, 16226, 596, 3158, 29889, 13, 13, 1529, 29934, 8426, 3308, 29901, 13, 1349, 852, 526, 896, 13, 7058, 1556, 526, 17762, 29889, 960, 738, 1316, 367, 1244, 489, 13, 2887, 372, 892, 4457, 304, 7404, 489, 5747, 5360, 445, 20413, 13, 11921, 262, 366, 1074, 592, 1560, 799, 29915, 29881, 29936, 565, 738, 8866, 13, 29931, 16136, 670, 2022, 1135, 385, 4486, 3461, 29936, 13, 3644, 738, 1348, 26565, 4892, 714, 705, 1141, 29879, 4319, 2834, 13, 2855, 393, 670, 4234, 29915, 29879, 9425, 261, 1135, 3654, 29936, 13, 12024, 1075, 7432, 29892, 470, 577, 1784, 577, 3458, 287, 29892, 13, 29956, 1351, 4550, 29892, 304, 4653, 670, 27963, 29892, 13, 2855, 1101, 1085, 16102, 29889, 13, 29949, 29892, 592, 7432, 29991, 29871, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]

 Decoded in :  nd balms applied to, you, yet dare I never
Deny your asking: take your choice of those
That best can aid your action.

MARCIUS:
Those are they
That most are willing. If any such be here--
As it were sin to doubt--that love this painting
Wherein you see me smear'd; if any fear
Lesser his person than an ill report;
If any think brave death outweighs bad life
And that his country's dearer than himself;
Let him alone, or so many so minded,
Wave thus, to express his disposition,
And follow Marcius.
O, me alone! </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>

 Decoded lbl: nd balms applied to, you, yet dare I never
Deny your asking: take your choice of those
That best can aid your action.

MARCIUS:
Those are they
That most are willing. If any such be here--
As it were sin to doubt--that love this painting
Wherein you see me smear'd; if any fear
Lesser his person than an ill report;
If any think brave death outweighs bad life
And that his country's dearer than himself;
Let him alone, or so many so minded,
Wave thus, to express his disposition,
And follow Marcius.
O, me alone! 

--- Example 2 ---
Input IDs  : [7432, 29889, 13, 27796, 29892, 325, 616, 29889, 13, 5618, 565, 445, 29544, 437, 451, 664, 472, 599, 29973, 13, 2713, 497, 306, 367, 8300, 769, 304, 29899, 26122, 7250, 29973, 13, 3782, 29892, 694, 29901, 445, 4091, 19752, 333, 372, 29901, 3804, 12595, 727, 29889, 13, 5618, 565, 372, 367, 263, 27908, 29892, 607, 278, 3484, 279, 13, 4035, 29873, 368, 23870, 11050, 29915, 29881, 304, 505, 592, 7123, 29892, 13, 29931, 342, 297, 445, 13718, 540, 881, 367, 270, 728, 265, 473, 29915, 29881, 29892, 13, 29933, 5658, 540, 8300, 592, 1434, 304, 9184, 29877, 29973, 13, 29902, 8866, 372, 338, 29901, 322, 3447, 29892, 286, 621, 19363, 29892, 372, 881, 451, 29892, 13, 2831, 540, 23870, 1603, 1063, 1898, 263, 26630, 767, 29889, 13, 5328, 565, 29892, 746, 306, 626, 12341, 964, 278, 20450, 29892, 13, 29902, 281, 1296, 1434, 278, 931, 393, 9184, 29877, 13, 27796, 304, 337, 311, 331, 592, 29973, 727, 29915, 29879, 263, 8866, 21154, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]

 Decoded in :  alone.
Come, vial.
What if this mixture do not work at all?
Shall I be married then to-morrow morning?
No, no: this shall forbid it: lie thou there.
What if it be a poison, which the friar
Subtly hath minister'd to have me dead,
Lest in this marriage he should be dishonour'd,
Because he married me before to Romeo?
I fear it is: and yet, methinks, it should not,
For he hath still been tried a holy man.
How if, when I am laid into the tomb,
I wake before the time that Romeo
Come to redeem me? there's a fearfu</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>

 Decoded lbl: alone.
Come, vial.
What if this mixture do not work at all?
Shall I be married then to-morrow morning?
No, no: this shall forbid it: lie thou there.
What if it be a poison, which the friar
Subtly hath minister'd to have me dead,
Lest in this marriage he should be dishonour'd,
Because he married me before to Romeo?
I fear it is: and yet, methinks, it should not,
For he hath still been tried a holy man.
How if, when I am laid into the tomb,
I wake before the time that Romeo
Come to redeem me? there's a fearfu

--- Example 3 ---
Input IDs  : [274, 4156, 29936, 13, 3492, 8829, 29891, 590, 3061, 27967, 322, 590, 7875, 2396, 13, 29954, 397, 16690, 591, 2360, 1122, 505, 817, 310, 366, 29991, 13, 13, 29954, 3927, 29965, 4741, 1254, 1001, 29901, 13, 6816, 424, 603, 29892, 4177, 867, 1934, 393, 591, 505, 817, 310, 366, 29901, 13, 10858, 8099, 338, 29535, 29915, 29881, 491, 596, 2794, 29892, 13, 29924, 952, 761, 766, 29887, 945, 287, 29892, 322, 278, 22182, 1793, 13, 29950, 2495, 297, 640, 3456, 29936, 21109, 1784, 6534, 2504, 327, 1080, 13, 17506, 14218, 2183, 304, 427, 1217, 569, 1906, 13, 7058, 21990, 346, 29892, 777, 1023, 3841, 1951, 29892, 892, 7088, 263, 15996, 29889, 13, 13, 11144, 1430, 382, 5265, 29999, 2882, 2544, 29950, 29901, 13, 2059, 18136, 393, 10425, 592, 304, 445, 16010, 3171, 13, 4591, 393, 2793, 287, 447, 29886, 607, 306, 13389, 29915, 29881, 29892, 13, 29902, 2360, 1258, 5528, 1947, 670, 10067, 14596, 13, 14769, 475, 303, 278, 13484, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]

 Decoded in :  cester;
You envy my advancement and my friends':
God grant we never may have need of you!

GLOUCESTER:
Meantime, God grants that we have need of you:
Your brother is imprison'd by your means,
Myself disgraced, and the nobility
Held in contempt; whilst many fair promotions
Are daily given to ennoble those
That scarce, some two days since, were worth a noble.

QUEEN ELIZABETH:
By Him that raised me to this careful height
From that contented hap which I enjoy'd,
I never did incense his majesty
Against the Duke</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>

 Decoded lbl: cester;
You envy my advancement and my friends':
God grant we never may have need of you!

GLOUCESTER:
Meantime, God grants that we have need of you:
Your brother is imprison'd by your means,
Myself disgraced, and the nobility
Held in contempt; whilst many fair promotions
Are daily given to ennoble those
That scarce, some two days since, were worth a noble.

QUEEN ELIZABETH:
By Him that raised me to this careful height
From that contented hap which I enjoy'd,
I never did incense his majesty
Against the Duke

wandb: Currently logged in as: chizhang-cs (chizhang-cs-the-university-of-texas-at-austin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /u/chizhang/Projects/DiscreteFlow/wandb/run-20250612_201541-kps36rrg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_tokenflow_baseline
wandb: ⭐️ View project at https://wandb.ai/chizhang-cs-the-university-of-texas-at-austin/TokenFlow
wandb: 🚀 View run at https://wandb.ai/chizhang-cs-the-university-of-texas-at-austin/TokenFlow/runs/kps36rrg
2025-06-12 20:15:42.462 | INFO     | __main__:main:331 - Model Configuration:
{'_attn_implementation_autoset': True,
 '_name_or_path': '',
 'add_cross_attention': False,
 'architectures': None,
 'bad_words_ids': None,
 'begin_suppress_tokens': None,
 'bos_token_id': None,
 'chunk_size_feed_forward': 0,
 'cross_attention_hidden_size': None,
 'ctx_len': 256,
 'decoder_start_token_id': None,
 'dim': 512,
 'diversity_penalty': 0.0,
 'do_sample': False,
 'early_stopping': False,
 'embed_scale': 0.044194173824159216,
 'encoder_no_repeat_ngram_size': 0,
 'eos_token_id': None,
 'exponential_decay_length_penalty': None,
 'finetuning_task': None,
 'forced_bos_token_id': None,
 'forced_eos_token_id': None,
 'hidden_dim': 2048,
 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},
 'init_cutoff_factor': 3.0,
 'is_decoder': False,
 'is_encoder_decoder': False,
 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},
 'length_penalty': 1.0,
 'load_stats': True,
 'max_batch': 64,
 'max_length': 20,
 'min_length': 0,
 'model_type': 'tokenflow',
 'multiple_of': 64,
 'n_heads': 8,
 'n_kv_heads': 8,
 'n_layers': 8,
 'no_repeat_ngram_size': 0,
 'norm_eps': 1e-06,
 'num_beam_groups': 1,
 'num_beams': 1,
 'num_return_sequences': 1,
 'output_attentions': False,
 'output_hidden_states': False,
 'output_scores': False,
 'pad_token_id': None,
 'prefix': None,
 'problem_type': None,
 'pruned_heads': {},
 'remove_invalid_values': False,
 'repetition_penalty': 1.0,
 'return_dict': True,
 'return_dict_in_generate': False,
 'rope_scaling': 10000,
 'sep_token_id': None,
 'suppress_tokens': None,
 'task_specific_params': None,
 'teacher_model_name': None,
 'temperature': 1.0,
 'tf_legacy_loss': False,
 'tie_encoder_decoder': False,
 'tie_word_embeddings': True,
 'time_dim': 256,
 'tokenizer_class': None,
 'top_k': 50,
 'top_p': 1.0,
 'torch_dtype': None,
 'torchscript': False,
 'transformers_version': '4.51.1',
 'typical_p': 1.0,
 'use_bfloat16': False,
 'use_causal': True,
 'use_gumbel_flow': False,
 'vocab_size': 32000}
2025-06-12 20:15:42.464 | INFO     | __main__:main:337 - HuggingFace Training Arguments:
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.98,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=False,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=2,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0004,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/u/chizhang/scratch/data/out_shakespeare_tokenflow/runs/Jun12_20-15-40_dgx-4,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=50,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_steps=10000,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/u/chizhang/scratch/data/out_shakespeare_tokenflow,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/u/chizhang/scratch/data/out_shakespeare_tokenflow,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=SaveStrategy.STEPS,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=1000,
weight_decay=0.02,
)
2025-06-12 20:15:42.464 | INFO     | __main__:main:338 - Total model parameters: 37771264
2025-06-12 20:15:42.464 | INFO     | __main__:main:339 - Non-embedding parameters: 21387264
2025-06-12 20:15:42.533 | INFO     | __main__:main:352 - Let the training begin.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|          | 0/10000 [00:00<?, ?it/s]  0%|          | 1/10000 [00:02<8:06:35,  2.92s/it]  0%|          | 2/10000 [00:04<5:51:25,  2.11s/it]  0%|          | 3/10000 [00:05<5:07:38,  1.85s/it]  0%|          | 4/10000 [00:07<4:47:27,  1.73s/it]  0%|          | 5/10000 [00:09<4:36:14,  1.66s/it]  0%|          | 6/10000 [00:10<4:29:47,  1.62s/it]  0%|          | 7/10000 [00:12<4:25:26,  1.59s/it]  0%|          | 8/10000 [00:13<4:22:52,  1.58s/it]  0%|          | 9/10000 [00:15<4:20:45,  1.57s/it]  0%|          | 10/10000 [00:16<4:19:24,  1.56s/it]  0%|          | 11/10000 [00:18<4:18:36,  1.55s/it]  0%|          | 12/10000 [00:19<4:17:56,  1.55s/it]  0%|          | 13/10000 [00:21<4:17:24,  1.55s/it]  0%|          | 14/10000 [00:22<4:17:20,  1.55s/it]  0%|          | 15/10000 [00:24<4:17:12,  1.55s/it]  0%|          | 16/10000 [00:26<4:16:55,  1.54s/it]  0%|          | 17/10000 [00:27<4:17:06,  1.55s/it]  0%|          | 18/10000 [00:29<4:17:01,  1.54s/it]  0%|          | 19/10000 [00:29<3:31:15,  1.27s/it]  0%|          | 20/10000 [00:31<3:54:11,  1.41s/it]  0%|          | 21/10000 [00:33<4:02:07,  1.46s/it]  0%|          | 22/10000 [00:34<4:06:31,  1.48s/it]  0%|          | 23/10000 [00:36<4:09:52,  1.50s/it]  0%|          | 24/10000 [00:37<4:11:44,  1.51s/it]  0%|          | 25/10000 [00:39<4:13:21,  1.52s/it]  0%|          | 26/10000 [00:40<4:14:19,  1.53s/it]  0%|          | 27/10000 [00:42<4:15:12,  1.54s/it]  0%|          | 28/10000 [00:43<4:15:38,  1.54s/it]  0%|          | 29/10000 [00:45<4:16:05,  1.54s/it]  0%|          | 30/10000 [00:46<4:16:15,  1.54s/it]  0%|          | 31/10000 [00:48<4:16:24,  1.54s/it]  0%|          | 32/10000 [00:50<4:16:41,  1.55s/it]  0%|          | 33/10000 [00:51<4:16:39,  1.55s/it]  0%|          | 34/10000 [00:53<4:16:18,  1.54s/it]  0%|          | 35/10000 [00:54<4:16:37,  1.55s/it]  0%|          | 36/10000 [00:56<4:16:42,  1.55s/it]  0%|          | 37/10000 [00:57<4:16:49,  1.55s/it]  0%|          | 38/10000 [00:58<3:32:08,  1.28s/it]  0%|          | 39/10000 [01:00<3:54:28,  1.41s/it]  0%|          | 40/10000 [01:01<4:01:07,  1.45s/it]  0%|          | 41/10000 [01:03<4:05:41,  1.48s/it]  0%|          | 42/10000 [01:04<4:09:01,  1.50s/it]  0%|          | 43/10000 [01:06<4:11:25,  1.52s/it]  0%|          | 44/10000 [01:07<4:12:57,  1.52s/it]  0%|          | 45/10000 [01:09<4:13:56,  1.53s/it]  0%|          | 46/10000 [01:10<4:14:55,  1.54s/it]  0%|          | 47/10000 [01:12<4:15:19,  1.54s/it]  0%|          | 48/10000 [01:14<4:15:40,  1.54s/it]  0%|          | 49/10000 [01:15<4:16:06,  1.54s/it]  0%|          | 50/10000 [01:17<4:16:32,  1.55s/it]                                                    {'loss': 2.6815, 'grad_norm': 1.2990325689315796, 'learning_rate': 1.9600000000000002e-05, 'epoch': 2.65}
  0%|          | 50/10000 [01:17<4:16:32,  1.55s/it]  1%|          | 51/10000 [01:18<4:16:30,  1.55s/it]  1%|          | 52/10000 [01:20<4:16:37,  1.55s/it]  1%|          | 53/10000 [01:21<4:16:32,  1.55s/it]  1%|          | 54/10000 [01:23<4:16:34,  1.55s/it]  1%|          | 55/10000 [01:24<4:16:45,  1.55s/it]  1%|          | 56/10000 [01:26<4:16:30,  1.55s/it]  1%|          | 57/10000 [01:27<3:31:58,  1.28s/it]  1%|          | 58/10000 [01:28<3:54:41,  1.42s/it]  1%|          | 59/10000 [01:30<4:02:17,  1.46s/it]  1%|          | 60/10000 [01:31<4:06:22,  1.49s/it]  1%|          | 61/10000 [01:33<4:09:32,  1.51s/it]  1%|          | 62/10000 [01:35<4:11:31,  1.52s/it]  1%|          | 63/10000 [01:36<4:13:01,  1.53s/it]  1%|          | 64/10000 [01:38<4:14:08,  1.53s/it]  1%|          | 65/10000 [01:39<4:14:50,  1.54s/it]  1%|          | 66/10000 [01:41<4:15:24,  1.54s/it]  1%|          | 67/10000 [01:42<4:15:49,  1.55s/it]  1%|          | 68/10000 [01:44<4:16:07,  1.55s/it]  1%|          | 69/10000 [01:45<4:16:15,  1.55s/it]  1%|          | 70/10000 [01:47<4:16:21,  1.55s/it]  1%|          | 71/10000 [01:49<4:16:38,  1.55s/it]  1%|          | 72/10000 [01:50<4:16:33,  1.55s/it]  1%|          | 73/10000 [01:52<4:16:21,  1.55s/it]  1%|          | 74/10000 [01:53<4:16:31,  1.55s/it]  1%|          | 75/10000 [01:55<4:16:17,  1.55s/it]  1%|          | 76/10000 [01:55<3:31:36,  1.28s/it]  1%|          | 77/10000 [01:57<3:53:22,  1.41s/it]  1%|          | 78/10000 [01:59<4:00:03,  1.45s/it]  1%|          | 79/10000 [02:00<4:04:31,  1.48s/it]  1%|          | 80/10000 [02:02<4:08:08,  1.50s/it]  1%|          | 81/10000 [02:03<4:10:28,  1.52s/it]  1%|          | 82/10000 [02:05<4:12:14,  1.53s/it]  1%|          | 83/10000 [02:06<4:13:30,  1.53s/it]  1%|          | 84/10000 [02:08<4:14:08,  1.54s/it]  1%|          | 85/10000 [02:10<4:14:51,  1.54s/it]  1%|          | 86/10000 [02:11<4:15:05,  1.54s/it]  1%|          | 87/10000 [02:13<4:15:23,  1.55s/it]  1%|          | 88/10000 [02:14<4:15:47,  1.55s/it]  1%|          | 89/10000 [02:16<4:15:55,  1.55s/it]  1%|          | 90/10000 [02:17<4:16:00,  1.55s/it]  1%|          | 91/10000 [02:19<4:15:29,  1.55s/it]  1%|          | 92/10000 [02:20<4:15:19,  1.55s/it]  1%|          | 93/10000 [02:22<4:15:25,  1.55s/it]  1%|          | 94/10000 [02:23<4:15:29,  1.55s/it]  1%|          | 95/10000 [02:24<3:31:04,  1.28s/it]  1%|          | 96/10000 [02:26<3:53:46,  1.42s/it]  1%|          | 97/10000 [02:27<4:00:31,  1.46s/it]  1%|          | 98/10000 [02:29<4:05:13,  1.49s/it]  1%|          | 99/10000 [02:30<4:08:31,  1.51s/it]  1%|          | 100/10000 [02:32<4:10:45,  1.52s/it]                                                     {'loss': 1.9975, 'grad_norm': 0.4741780757904053, 'learning_rate': 3.960000000000001e-05, 'epoch': 5.27}
  1%|          | 100/10000 [02:32<4:10:45,  1.52s/it]  1%|          | 101/10000 [02:34<4:12:16,  1.53s/it]  1%|          | 102/10000 [02:35<4:13:14,  1.54s/it]  1%|          | 103/10000 [02:37<4:14:04,  1.54s/it]  1%|          | 104/10000 [02:38<4:14:36,  1.54s/it]  1%|          | 105/10000 [02:40<4:14:58,  1.55s/it]  1%|          | 106/10000 [02:41<4:15:11,  1.55s/it]  1%|          | 107/10000 [02:43<4:15:16,  1.55s/it]  1%|          | 108/10000 [02:44<4:15:22,  1.55s/it]  1%|          | 109/10000 [02:46<4:14:56,  1.55s/it]  1%|          | 110/10000 [02:48<4:15:13,  1.55s/it]  1%|          | 111/10000 [02:49<4:15:09,  1.55s/it]  1%|          | 112/10000 [02:51<4:15:28,  1.55s/it]  1%|          | 113/10000 [02:52<4:15:26,  1.55s/it]  1%|          | 114/10000 [02:53<3:30:43,  1.28s/it]  1%|          | 115/10000 [02:55<3:52:44,  1.41s/it]  1%|          | 116/10000 [02:56<3:59:37,  1.45s/it]  1%|          | 117/10000 [02:58<4:04:22,  1.48s/it]  1%|          | 118/10000 [02:59<4:07:40,  1.50s/it]  1%|          | 119/10000 [03:01<4:09:45,  1.52s/it]  1%|          | 120/10000 [03:02<4:11:28,  1.53s/it]  1%|          | 121/10000 [03:04<4:12:28,  1.53s/it]  1%|          | 122/10000 [03:05<4:13:20,  1.54s/it]  1%|          | 123/10000 [03:07<4:13:55,  1.54s/it]  1%|          | 124/10000 [03:09<4:14:11,  1.54s/it]  1%|▏         | 125/10000 [03:10<4:14:31,  1.55s/it]  1%|▏         | 126/10000 [03:12<4:14:46,  1.55s/it]  1%|▏         | 127/10000 [03:13<4:15:00,  1.55s/it]  1%|▏         | 128/10000 [03:15<4:15:03,  1.55s/it]  1%|▏         | 129/10000 [03:16<4:15:04,  1.55s/it]  1%|▏         | 130/10000 [03:18<4:15:00,  1.55s/it]  1%|▏         | 131/10000 [03:19<4:15:04,  1.55s/it]  1%|▏         | 132/10000 [03:21<4:14:54,  1.55s/it]  1%|▏         | 133/10000 [03:22<3:29:58,  1.28s/it]  1%|▏         | 134/10000 [03:23<3:52:05,  1.41s/it]  1%|▏         | 135/10000 [03:25<3:59:57,  1.46s/it]  1%|▏         | 136/10000 [03:26<4:04:06,  1.48s/it]  1%|▏         | 137/10000 [03:28<4:07:21,  1.50s/it]  1%|▏         | 138/10000 [03:30<4:09:33,  1.52s/it]  1%|▏         | 139/10000 [03:31<4:10:56,  1.53s/it]  1%|▏         | 140/10000 [03:33<4:12:02,  1.53s/it]  1%|▏         | 141/10000 [03:34<4:12:47,  1.54s/it]  1%|▏         | 142/10000 [03:36<4:13:21,  1.54s/it]  1%|▏         | 143/10000 [03:37<4:13:46,  1.54s/it]  1%|▏         | 144/10000 [03:39<4:13:56,  1.55s/it]  1%|▏         | 145/10000 [03:40<4:14:11,  1.55s/it]  1%|▏         | 146/10000 [03:42<4:14:14,  1.55s/it]  1%|▏         | 147/10000 [03:43<4:14:18,  1.55s/it]  1%|▏         | 148/10000 [03:45<4:14:25,  1.55s/it]  1%|▏         | 149/10000 [03:47<4:14:13,  1.55s/it]  2%|▏         | 150/10000 [03:48<4:14:28,  1.55s/it]                                                     {'loss': 1.795, 'grad_norm': 0.4176522195339203, 'learning_rate': 5.96e-05, 'epoch': 7.92}
  2%|▏         | 150/10000 [03:48<4:14:28,  1.55s/it]  2%|▏         | 151/10000 [03:50<4:14:35,  1.55s/it]  2%|▏         | 152/10000 [03:50<3:29:35,  1.28s/it]  2%|▏         | 153/10000 [03:52<3:51:51,  1.41s/it]  2%|▏         | 154/10000 [03:54<3:59:37,  1.46s/it]  2%|▏         | 155/10000 [03:55<4:03:48,  1.49s/it]  2%|▏         | 156/10000 [03:57<4:06:47,  1.50s/it]  2%|▏         | 157/10000 [03:58<4:09:02,  1.52s/it]  2%|▏         | 158/10000 [04:00<4:10:29,  1.53s/it]  2%|▏         | 159/10000 [04:01<4:11:33,  1.53s/it]  2%|▏         | 160/10000 [04:03<4:12:13,  1.54s/it]  2%|▏         | 161/10000 [04:04<4:12:40,  1.54s/it]  2%|▏         | 162/10000 [04:06<4:12:57,  1.54s/it]  2%|▏         | 163/10000 [04:08<4:13:13,  1.54s/it]  2%|▏         | 164/10000 [04:09<4:13:11,  1.54s/it]  2%|▏         | 165/10000 [04:11<4:13:09,  1.54s/it]  2%|▏         | 166/10000 [04:12<4:13:26,  1.55s/it]  2%|▏         | 167/10000 [04:14<4:13:20,  1.55s/it]  2%|▏         | 168/10000 [04:15<4:13:24,  1.55s/it]  2%|▏         | 169/10000 [04:17<4:13:31,  1.55s/it]  2%|▏         | 170/10000 [04:18<4:13:24,  1.55s/it]  2%|▏         | 171/10000 [04:19<3:29:06,  1.28s/it]  2%|▏         | 172/10000 [04:21<3:50:59,  1.41s/it]  2%|▏         | 173/10000 [04:22<3:57:44,  1.45s/it]  2%|▏         | 174/10000 [04:24<4:02:18,  1.48s/it]  2%|▏         | 175/10000 [04:25<4:05:27,  1.50s/it]  2%|▏         | 176/10000 [04:27<4:07:27,  1.51s/it]  2%|▏         | 177/10000 [04:28<4:09:12,  1.52s/it]  2%|▏         | 178/10000 [04:30<4:10:20,  1.53s/it]  2%|▏         | 179/10000 [04:32<4:11:09,  1.53s/it]  2%|▏         | 180/10000 [04:33<4:11:56,  1.54s/it]  2%|▏         | 181/10000 [04:35<4:12:10,  1.54s/it]  2%|▏         | 182/10000 [04:36<4:12:17,  1.54s/it]  2%|▏         | 183/10000 [04:38<4:12:10,  1.54s/it]  2%|▏         | 184/10000 [04:39<4:12:14,  1.54s/it]  2%|▏         | 185/10000 [04:41<4:12:24,  1.54s/it]  2%|▏         | 186/10000 [04:42<4:12:23,  1.54s/it]  2%|▏         | 187/10000 [04:44<4:12:36,  1.54s/it]  2%|▏         | 188/10000 [04:45<4:12:51,  1.55s/it]  2%|▏         | 189/10000 [04:47<4:12:53,  1.55s/it]  2%|▏         | 190/10000 [04:48<3:28:30,  1.28s/it]  2%|▏         | 191/10000 [04:49<3:53:02,  1.43s/it]  2%|▏         | 192/10000 [04:51<3:59:47,  1.47s/it]  2%|▏         | 193/10000 [04:53<4:03:37,  1.49s/it]  2%|▏         | 194/10000 [04:54<4:06:24,  1.51s/it]  2%|▏         | 195/10000 [04:56<4:08:19,  1.52s/it]  2%|▏         | 196/10000 [04:57<4:09:39,  1.53s/it]  2%|▏         | 197/10000 [04:59<4:10:27,  1.53s/it]  2%|▏         | 198/10000 [05:00<4:11:03,  1.54s/it]  2%|▏         | 199/10000 [05:02<4:11:30,  1.54s/it]  2%|▏         | 200/10000 [05:03<4:11:50,  1.54s/it]                                                     {'loss': 1.5528, 'grad_norm': 0.3059156537055969, 'learning_rate': 7.960000000000001e-05, 'epoch': 10.54}
  2%|▏         | 200/10000 [05:03<4:11:50,  1.54s/it]  2%|▏         | 201/10000 [05:05<4:12:10,  1.54s/it]  2%|▏         | 202/10000 [05:06<4:12:11,  1.54s/it]  2%|▏         | 203/10000 [05:08<4:12:20,  1.55s/it]  2%|▏         | 204/10000 [05:10<4:12:11,  1.54s/it]  2%|▏         | 205/10000 [05:11<4:12:02,  1.54s/it]  2%|▏         | 206/10000 [05:13<4:11:56,  1.54s/it]  2%|▏         | 207/10000 [05:14<4:12:00,  1.54s/it]  2%|▏         | 208/10000 [05:16<4:12:01,  1.54s/it]  2%|▏         | 209/10000 [05:16<3:28:01,  1.27s/it]  2%|▏         | 210/10000 [05:18<3:49:13,  1.40s/it]  2%|▏         | 211/10000 [05:20<3:57:18,  1.45s/it]  2%|▏         | 212/10000 [05:21<4:01:24,  1.48s/it]